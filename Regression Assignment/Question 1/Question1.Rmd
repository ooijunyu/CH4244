---
title: "R Notebook"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

# Question 1

## Importing necessary library

```{r}
library(tidyverse)
library(car)
library(caret)
library(scatterplot3d)
library(factoextra)
```

## Importing data

First, the data set is imported into the notebook.

```{r}
data <- read_csv("Advertising.csv")
attach(data)
```

## Data inspection

Then we have a quick inspect in the data set.

```{r}
str(data)
summary(data)
```

Plot a histogram of `Sales`, `TV`, `radio` and `newspaper` individually

```{r}
par(mfrow = c(2,2))
hist(TV,breaks = seq(0,300,30))
hist(radio)
hist(newspaper)
hist(Sales)
```

The spread of data in `TV` seems to be even. However, in `radio` and `newspaper`, there are higher frequency of lower value investment with `newspaper` having a more obvious trend. From the histogram, we observed that `Sales` are roughly normally distributed with slight skewed to the right.

Next, we plot scatter plot of `Sales` vs `TV`, `radio`, and `newspaper` individually.

```{r}
par(mfrow = c(1,3))
plot(TV, Sales, main = "Sales vs TV")
plot(radio, Sales, main = "Sales vs radio")
plot(newspaper, Sales, main = "Sales vs newspaper")
```

## Data pre-processing

Now we randomly shuffle the data.

```{r}
set.seed(100) # To produce reproducible result

data <- data %>% 
  select(Sales,TV,radio,newspaper) %>% 
  mutate(rand = runif(dim(data)[1]),) %>% 
  arrange(rand)

head(data)
```

From the scatter plot, we observe that `Sales` is a concave function of `TV`. Hence, we will try to fit the data with concave function such as $\sqrt{TV}$. The relationship between `Sales` and `Radio` is roughly linear observing from the graph while there seems to not have any correlation between `Sales` and `Newspaper`. We will compute a correlation matrix to find out.

```{r}
cor(select(data,-rand))
```

From the correlation matrix, we can see that `Sales` is highly correlated to `TV` and `Radio` with $r = 0.782$ and $r = 0.576$ respectively, while `Sales` is weakly correlated with `Newspaper` with $r = 0.228$. Do note that the predictors `Radio` and `Newspaper` are correlated to each other with $r = 0.354$. Hence, we expect only one of either will make a good predictor.

## Repeated K-fold Cross Validation

### All predictors

Now we will use repeated K-fold cross validation (K = 10) and fit with a linear regression $\hat{Sales}=\beta_0+\beta_1\cdot\sqrt{TV}+\beta_2\cdot radio+\beta_3\cdot newspaper$

```{r}
# First we set training control to repeated K-fold cross validation 
# with K = 10, and repeats = 3
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)

# Then we train the linear model
model1 <- train(Sales ~ I(sqrt(TV)) + radio + newspaper, data = data, method = "lm",
               trControl = train.control)

# Summary of the result
summary(model1)
print(model1)
```

From the statistics we have, $R^2_{adj}=0.9279$ and $F=854.3$ ($p<2.2\times10^{-16}$), this is considered a good model.

However, when we look closely into the t-values of the predictors, we noticed that both `sqrt(TV)` and `radio` are significant with $p<2\times10^{-16}$ while `newspaper` is not significant with $p = 0.914$.

The coefficients of the predictors told us that for every 1 additional unit of `sqrt(TV)` invested, 0.975 unit of `Sales` will be generated; for every 1 additional unit of `radio` invested, 0.195 unit of `Sales` will be generated; for every 1 additional unit of `newspaper` invested, -0.0005 unit of `Sales` will be generated, which is a loss.

Now, we will use Variance Inflation Factor(VIF) to check for multicollinearity.

```{r}
vif(model1$finalModel)
```

From the VIF generated, the $VIF_{\sqrt{TV}}\approx1$, $VIF_{radio}=1.14$ and $VIF_{newspaper}=1.14$ shows that there is some collinearity between the predictors `radio` and `newspaper`.

### Without newspaper

Next we try to train the model without the predictor `newspaper`.

```{r}
# Model without newspaper predictor
model2 <- train(Sales ~ I(sqrt(TV)) + radio, data = data, method = "lm",
               trControl = train.control)

# Summary of the result
summary(model2)
print(model2)
```

We can see the new model without the predictor `newspaper` has a higher $F=1288$ and $R^2_{adj}=0.9282$, which indicates that this is a slightly better model compared to the model with `newspaper`. We will do an ANOVA test to confirm.

```{r}
anova(model1$finalModel, model2$finalModel)
```

However, the ANOVA test has a $p=0.9144$, which means that we cannot reject the null hypothesis that `model1` and `model2` fits the data equally well. There is no sufficient evidence that the model without the `newspaper` feature is significantly better.

Of course, we will check again the VIF of the predictors for any multicollinearity.

```{r}
vif(model2$finalModel)
```

We can see both VIF values are approximate equal to 1, which indicates that there are no multicollinearity between the predictors.

Next up, we will plot the residual graphs to inspect how are the residuals distributed.

```{r}
par(mfrow = c(2,2))
plot(model2$finalModel)
```

We can see that the residuals are not randomly distributed across the 0 abline. This indicates that there are some bias going on with current model. We now try out the 3D scatter plot of the raw and predicted data to inspect.

```{r}
s3d <- scatterplot3d(x = sqrt(TV), y = radio, z = Sales, angle = 30)

# adding in the prediction plane
s3d$plane3d(model2$finalModel)
```

We can see that at the extreme value(pure input) of either `radio` or `sqrt(TV)`, the model underestimate the data, while when there is a mixed input from both predictors, the model overestimate the data. It clearly shows that there are some synergy or interaction between the 2 predictors term. Hence, we will fit again a model with interaction term between the 2 predictors.

### Adding interaction variables

$\hat{Sales}=\beta_0+\beta_1\cdot\sqrt{TV}+\beta_2\cdot radio+\beta_3\cdot\sqrt{TV}\times radio$

```{r}
# Model with interaction term
model3 <- train(Sales ~ I(sqrt(TV)) * radio, data = data, method = "lm",
               trControl = train.control)

# Summary of the result
summary(model3)
print(model3)
```

We can see that the new model has a $R^2_{adj}=0.993$ and $F=8949$ which is way higher than the previous 2 models. We will do ANOVA test to compare this model with `model2`.

```{r}
# Comparing with model2
anova(model2$finalModel, model3$finalModel)
```

We can see that the $p<2.2\times 10^{-16}$ of the ANOVA test rejects the null hypothesis that `model2` and `model3` fits equally well. Hence, there is evidence that the model with interaction term fits significantly better than then the model without the interaction term.

We can also plot the residual plot to see how the residuals are distributed.

```{r}
par(mfrow = c(2,2))
plot(model3$finalModel)
```

Now we can see that the residuals are roughly randomly distributed across the 0 abline.

## Principal Component Analysis

Now we are going to do Principle Component Analysis(PCA) with a variance threshold of 95% and plot out the scree plot.

There are 2 methods for PCA - spectral decomposition and singular value decomposition. We will try both methods and compare the results

```{r}
# PCA spectral decomposition method
pca.spectral <- princomp(select(data,TV,radio,newspaper))
summary(pca.spectral)

# Scree plot
fviz_eig(pca.spectral)

# Obtaining the eigenvalues and var explained
get_eig(pca.spectral)

# The contributions of each features in each component 
pca.spectral.var <- get_pca_var(pca.spectral)
print(round(pca.spectral.var$contrib,5))

# The coefficients of each features in each component
pca.spectral$loadings
```

With the spectral decomposition approach, we can see that the 1st component represents roughly 91.4% of the variance, 2nd component represents roughly 6.4% of the the variance, and the 3rd component represents roughly 2.2% of the variance. Hence, with a variance threshold of 95%, 2 components will be selected, and roughly 97.8% of the variance are explained by the 2 components.

Next is the PCA with SVD method.

```{r}
# PCA SVD method
pca.svd <- prcomp(select(data,TV,radio,newspaper), 
                         scale. = TRUE, center = TRUE)
summary(pca.svd)

# Scree plot
fviz_eig(pca.svd)

# Obtaining the eigenvalues and var explained
get_eig(pca.svd)

# The contribution of each features in each component 
pca.svd.var <- get_pca_var(pca.svd)
print(round(pca.svd.var$contrib,5))

# The coefficients of each features in each component
pca.svd$rotation
```

However, with the singular value decomposition approach, we can see that the 1st component represents roughly 45.7% of the variance, 2nd component represents roughly 32.8% of the the variance, and the 3rd component represents roughly 21.5% of the variance. Hence, with a variance threshold of 95%, all components will be selected, and 100% of the variance are explained by the 3 components.

__Note:__ When I compare my result with my friends that compute using MATLAB, I realise that MATLAB is using Spectral Decomposition method, where 2 components will be selected with 95% variance threshold. 




















